---

layout:     post
title:      "Task-Oriented Dailogue Pre-Trained Model"
subtitle:   TODè°ƒç ”ï¼
date:       2022-07-17 21:00:00
author:     "Andrew Zeng"
tags:

  - å¯¹è¯ç”Ÿæˆ
  - Knowledge-Grounded Dialogue
---

è°ƒç ”äº†å¸¸è§çš„Task-Orientedå¯¹è¯é¢„è®­ç»ƒæ¨¡å‹ï¼ŒåŒ…æ‹¬TOD-BERTï¼ŒUBARï¼ŒPLATOç³»åˆ—ï¼ŒPPTODï¼ŒGALAXYä»¥åŠSPACE.

## TOD-BERT

https://www.semanticscholar.org/paper/TOD-BERT%3A-Pre-trained-Natural-Language-for-Dialogue-Wu-Hoi/5b015296730273921889e54a0a31e3b173017026

### 1. Year

2020.10

### 2. BackBone

BERT (ç”±BERT-base-uncasedæ¨¡å‹åˆå§‹åŒ–)

TOD-BERTä»£ç ï¼šhttps://github.com/jasonwu0731/ToD-BERT

åŒæ—¶å¼€æºäº†æƒé‡

### 3. Training Data

è‹±æ–‡æ•°æ®

9 nine different task-oriented datasets which are English, human-human and multi-turn.

In total, there are 100,707 dialogues, which contain 1,388,152 utterances over 60 domains.

<img width="675" alt="æˆªå±2022-07-17 ä¸‹åˆ1 59 30" src="https://user-images.githubusercontent.com/47687248/179397434-39069ea4-7701-4e00-b694-490c56266f97.png">

### 4. Training

å°†å¯¹è¯$D=\{S_{1},U_{1},\cdots,S_{n},U_{n}\}$, æ‹¼æ¥ä¸º$[SYS]S_{1}[USR]U_{1}\cdots$, ä»ç„¶ä½¿ç”¨æ ‡å‡†çš„position embeddingsä»¥åŠsegmentation embeddings.

<img width="565" alt="æˆªå±2022-07-17 ä¸‹åˆ2 13 58" src="https://user-images.githubusercontent.com/47687248/179397555-544bc4c7-18a8-470d-a7c8-e250e83522ff.png">

è®­ç»ƒç­–ç•¥é‡‡ç”¨äº†BERTçš„Masked Language modelingï¼ˆMLMï¼‰ä»¥åŠResponse contrastive lossï¼ˆRCLï¼‰ã€‚

1. MLM
2. RCLåˆ™å¯¹äºgolden responseä»¥åŠåŒä¸€ä¸ªbatchä¸­çš„å…¶ä»–responseçš„[CLS]çš„respresentationä½¿ç”¨å¯¹æ¯”å­¦ä¹ 

æ³¨ï¼šç”±äºåœ¨intentåˆ†ç±»çš„è¿‡ç¨‹ä¸­éœ€è¦[CLS]çš„è¡¨ç¤ºï¼Œä½¿ç”¨æ— ç›‘ç£çš„æ–¹æ³•è®­ç»ƒ[CLS]çš„è¡¨ç¤ºæ˜¯ä¸ªå¯ä»¥åŠªåŠ›çš„æ–¹å‘

### 5. Finetune Task

Intent Recognition; Dialogue State Tracking; Dialogue Act Prediction; Response Selection

## UBAR

https://www.semanticscholar.org/paper/UBAR%3A-Towards-Fully-End-to-End-Task-Oriented-Dialog-Yang-Li/63169665bd592fb818678c47644b29302877d50e

### 1. Year

2021.03

### 2. BackBone

GPT-2

UBARä»£ç ï¼šhttps://github.com/TonyNemo/UBAR-MultiWOZ

åŒæ—¶å¼€æºäº†æƒé‡

### 3. Training Data

åªæœ‰finetuneé˜¶æ®µMultiWOZ 2.0&2.1

#### 4. Training

<img width="743" alt="æˆªå±2022-07-17 ä¸‹åˆ7 25 37" src="https://user-images.githubusercontent.com/47687248/179397915-53227a11-d7d1-4864-a8fd-b9711b03fc31.png">

#### 5. Finetune Task

End-to-end Modeling; Dialog State Tracking

## PPTOD

https://www.semanticscholar.org/paper/Multi-Task-Pre-Training-for-Plug-and-Play-Dialogue-Su-Shu/841ca3d87a422451596b8a4b8350e92106971791

### 1. Year

2022.03

### 2. BackBone

T5 (åŒ…æ‹¬small,base,large)

### 3. Training Data

In total, there are over 2.3M utterances across 80 domains.
 
PPTODä»£ç ï¼šhttps://github.com/awslabs/pptod

æƒé‡å·²å¼€æº

### 4. Training

å°†å¤šä¸ªTODä»»åŠ¡ç»Ÿä¸€ä¸ºText-to-Textçš„å½¢å¼

<img width="831" alt="æˆªå±2022-07-17 ä¸‹åˆ6 36 49" src="https://user-images.githubusercontent.com/47687248/179398248-1a64c928-0f73-4a06-b8d7-d3b3bf72138f.png">

### 5. Finetune Task

Intent Recognition; End-to-end Modeling; Dialog State Tracking

## PLATO

PLATOå…¶å®å¼€æ”¾åŸŸçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚

https://aclanthology.org/2020.acl-main.9.pdf

### 1. Year

2020.04

### 2. BackBone

UniLM ï¼ˆsupports both bi-directional encoding anduni-directional decodingï¼‰ æƒé‡ç”±$BERT_{Base}$åˆå§‹åŒ–å¾—åˆ° 132M

PLATOä»£ç ï¼šhttps://github.com/PaddlePaddle/Research/tree/master/NLP/Dialogue-PLATO

æƒé‡å·²å¼€æº

### 3. Training Data

Large-scale conversation datasets â€“Twitter (Cho et al., 2014) and Reddit (Zhou et al.,2018; Galley et al., 2019) are employed for pre-training, which results in 8.3 million training samples in total. (è®­äº†ä¸¤å‘¨)

### 4. Training

è¾“å…¥å†…å®¹åŒ…æ‹¬latent variableï¼ˆz); dialogue contextä»¥åŠresponse.  å…¶ä¸­zçš„token embedding $E_{z}$ç›´æ¥ç”±latent embedding sapceå¾—åˆ°ã€‚

è®­ç»ƒç­–ç•¥åŒ…æ‹¬negative log-likelihood(NLL)loss,bag-of-words(BOW)lossä»¥åŠresponse selec-tion(RS)loss.

1. NNL loss 
2. BOW loss ï¼ˆä½¿ç”¨latent variableä»¥åŠcontextå»é¢„æµ‹responseä¸­çš„wordï¼‰
3. RS loss (helps distinguish whether theresponse is relevant with the dialogue context andconsistent with the background knowledge)

<img width="824" alt="æˆªå±2022-07-17 ä¸‹åˆ3 36 09" src="https://user-images.githubusercontent.com/47687248/179398534-634cb7d4-29ec-4a6c-8df7-708daca4b7e7.png">

### 5. Finetune Task

End-to-end Modeling

## PLATO-2

### 1. Year

2021.05

### 2. BackBone

UniLMæ¶æ„ 1.6B

PLATO-2ä»£ç ï¼šhttps://github.com/PaddlePaddle/Knover/tree/develop/projects/PLATO-2

è‹±æ–‡æƒé‡å·²å¼€æº

### 3. Training Data

The training set contains 811M (context, response) samples, ranging from December 2005 to December 2019. For the validation set, 0.2M samples are selected from the rest data after December2019. The English vocabulary contains 8K BPE tokens (Sennrich et al., 2016), constructed with theSentencePiece library. The Chinese pre-training data is collected from public domain social medias.After filtering, there are 1.2B (context, response) samples in the training set.  As for the Chinese vocabulary, it contains 30K BPE tokens.

### 4. Training

<img width="802" alt="æˆªå±2022-07-17 ä¸‹åˆ5 45 52" src="https://user-images.githubusercontent.com/47687248/179398769-3a2e8ba9-7380-4fe9-bdf7-9d6fb62da641.png">

### 5. Fintune Task

End-to-end Modeling

## PLATO-XL

### 1. Year

2021.09

### 2. BackBone

UniLMæ¶æ„ 11B

<img width="720" alt="æˆªå±2022-07-17 ä¸‹åˆ4 49 44" src="https://user-images.githubusercontent.com/47687248/179398816-c1c37e89-7225-48de-b735-ea7198449fe1.png">

PLATO-XLä»£ç ï¼šhttps://github.com/PaddlePaddle/Knover/blob/develop/projects/PLATO-XL/README.md

è‹±æ–‡æƒé‡å·²å¼€æº

### 3. Training Data

åŒä¸Š

### 4. Training

è§„æ¨¡ä¸Šæ¥äº†ä»¥åå°±NLL Losså³å¯

<img width="723" alt="æˆªå±2022-07-17 ä¸‹åˆ5 05 34" src="https://user-images.githubusercontent.com/47687248/179398911-06900238-18db-4a37-954c-7626760e44f8.png">

### 5. Finetune Task

End-to-end Modeling; Dialog State Tracking

## GALAXY

ç»è¿‡ä¸PLATOçš„æ¯”è¾ƒï¼ŒGALAXYæ›´åƒæ˜¯æ‹¥æœ‰æ ‡ç­¾æ•°æ®çš„ç‰ˆæœ¬ã€‚å…·ä½“è€Œè¨€ï¼Œä½œè€…æ„é€ äº†æœ‰æ ‡ç­¾çš„æ•°æ®ï¼Œå¦‚dialogue actæœ‰äº†æ ‡ç­¾ã€‚

https://www.semanticscholar.org/paper/GALAXY%3A-A-Generative-Pre-trained-Model-for-Dialog-He-Dai/127ffc8697630a76b1b4149c24d1350f69205f41

### 1. Year

2022.03

### 2. BackBone

Unilm

GALAXYä»£ç ï¼šhttps://github.com/siat-nlp/GALAXY

### 3. Training Data

UniDAï¼ˆLabeled Datasetï¼‰ï¼šä½¿ç”¨ç»Ÿä¸€çš„DA taxonnmyåšTODä»»åŠ¡ã€‚ UniDAï¼ˆUnlabled Datasetï¼‰

<img width="309" alt="æˆªå±2022-07-17 ä¸‹åˆ7 37 44" src="https://user-images.githubusercontent.com/47687248/179399043-5351652e-28ae-441e-9cf3-555b790980ca.png">

### 4. Training

ä¸ç¬¬ä¸€ä»£PLATOéå¸¸ç›¸ä¼¼

<img width="709" alt="æˆªå±2022-07-17 ä¸‹åˆ7 38 23" src="https://user-images.githubusercontent.com/47687248/179399060-53176be6-03f4-4c71-9a10-b87460ee7063.png">

### 5. Finetune Task

End-to-end Modeling

## SPACE

ç›¸æ¯”äºGALAXYï¼ŒåŠ å…¥äº†Promptï¼ŒåŒæ—¶åˆ†ä¸ºå¤šä¸ªmoduleï¼Œå¯ä»¥æ‰¿æ‹…ä¸åŒçš„ä»»åŠ¡

https://www.semanticscholar.org/paper/Unified-Dialog-Model-Pre-training-for-Task-Oriented-He-Dai/553703db1b3e54e957ed91bad952ff3ba4f59bd5

### 1. Year

2022.07

### 2. BackBone

Unilm

### 3. Training Data

<img width="346" alt="æˆªå±2022-07-17 ä¸‹åˆ7 43 31" src="https://user-images.githubusercontent.com/47687248/179399185-d0ffc023-3e84-4247-bbeb-4da3939eb905.png">

### 4. Training

åŸºäºæœ‰æ ‡ç­¾æ•°æ®å’Œæ— æ ‡ç­¾æ•°æ®è®¾è®¡äº†å¤šä»»åŠ¡è®­ç»ƒç›®æ ‡ã€‚

<img width="733" alt="æˆªå±2022-07-17 ä¸‹åˆ7 45 24" src="https://user-images.githubusercontent.com/47687248/179399272-d67d0dbd-8cb3-44ea-830e-7de082c11c78.png">

### 5. Finetune Task

Intent Recognition; End-to-end Modeling; Dialog State Tracking

å…·ä½“è€Œè¨€ï¼š

For dialog understanding tasks, we only leveragethe dialog encoding module and the dialog understanding moduleto fine-tune our pre-trained model, discarding the dialog policy andgeneration modules. Thus only the understanding prompt sequenceğ‘ğ‘¢is employed to extract sentence representations, which are usedfor classification tasks such as intent recognition or dialog statetracking. For policy optimization and dialog generation tasks, weretain the integrated model architecture to produce appropriateresponses, where two kinds of prompts $p^{u}$and $p^{o}$ are adopted tocharacterize the dialog understanding and dialog policy for betterdialog generation.








